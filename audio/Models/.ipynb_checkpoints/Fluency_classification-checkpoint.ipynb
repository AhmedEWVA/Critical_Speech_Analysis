{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RG7EHYuTHvyN"
   },
   "source": [
    "# Importing packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "az7GZszAlBuc"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import os\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn import svm\n",
    "import pickle\n",
    "\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "import math\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.lines as mlines\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# to play the audio files\n",
    "from IPython.display import Audio\n",
    "\n",
    "# Neural networks can be constructed using the torch.nn package.\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torchvision import models,transforms,datasets\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MWvSg3XS6ufQ"
   },
   "outputs": [],
   "source": [
    "import pickle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qZvuqjjTlCOZ"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LRDZdqlEt9nj"
   },
   "source": [
    "# Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5rZ-VMOblCZO"
   },
   "outputs": [],
   "source": [
    "dir = \"drive/MyDrive/Colab_Notebooks/mit_data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mRtD8-v-lCjz"
   },
   "outputs": [],
   "source": [
    "dataset_df = pd.read_csv(dir+\"/features_fluency.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2gKbBdPklCtm"
   },
   "outputs": [],
   "source": [
    "dataset_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fOBx3QdTlC_g"
   },
   "outputs": [],
   "source": [
    "columns_with_nan_values = []\n",
    "for clmn in dataset_df.columns:\n",
    "  num = len(dataset_df[clmn].values[dataset_df[clmn].isnull()])\n",
    "  if (num !=0):\n",
    "    columns_with_nan_values.append(clmn)\n",
    "print(len(columns_with_nan_values))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ENwMUkXMlDFp"
   },
   "outputs": [],
   "source": [
    "dataset_df.dropna(subset = columns_with_nan_values, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LDnITLjdlDKj"
   },
   "outputs": [],
   "source": [
    "columns_with_nan_values = []\n",
    "for clmn in dataset_df.columns:\n",
    "  num = len(dataset_df[clmn].values[dataset_df[clmn].isnull()])\n",
    "  if (num !=0):\n",
    "    columns_with_nan_values.append(clmn)\n",
    "print(len(columns_with_nan_values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QJRpzSOUvJ_g"
   },
   "outputs": [],
   "source": [
    "X = dataset_df.iloc[: ,:-1].values\n",
    "Y = dataset_df['labels'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "buCphpLeuXoU"
   },
   "outputs": [],
   "source": [
    "print(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kszZ1GFOvJws"
   },
   "outputs": [],
   "source": [
    "# As this is a multiclass classification problem onehotencoding our Y.\n",
    "encoder = OneHotEncoder()\n",
    "Y = encoder.fit_transform(np.array(Y).reshape(-1,1)).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "48dFFLINvW3q"
   },
   "outputs": [],
   "source": [
    "print(Y)\n",
    "print(len(Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ni7VpxGlvdY0"
   },
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, random_state=0, shuffle=True)\n",
    "x_train.shape, y_train.shape, x_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7umkKq0mvJQ1"
   },
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "x_train = scaler.fit_transform(x_train)\n",
    "x_test = scaler.transform(x_test)\n",
    "x_train.shape, y_train.shape, x_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zvC5W5D3wRyl"
   },
   "source": [
    "# Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wn0lJYv3tuFp"
   },
   "source": [
    "## CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1TMs1oROlDaG"
   },
   "outputs": [],
   "source": [
    "x_train_tensor = torch.tensor(x_train,dtype=torch.float32)\n",
    "x_test_tensor = torch.tensor(x_test,dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train,dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test,dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Gg_XPSktlDoe"
   },
   "outputs": [],
   "source": [
    "class my_dataset_train(Dataset):\n",
    "\n",
    "    def __init__(self):\n",
    "        # Initialize data, download, etc.\n",
    "        # read with numpy or pandas\n",
    "        #xy = np.loadtxt('./data/wine/wine.csv', delimiter=',', dtype=np.float32, skiprows=1)\n",
    "\n",
    "        self.n_samples = x_train.shape[0]\n",
    "        \n",
    "        # here the first column is the class label, the rest are the features\n",
    "        self.x_data = torch.tensor(x_train,dtype=torch.float32) # size [n_samples, n_features]\n",
    "        self.y_data = torch.tensor(y_train,dtype=torch.float32) # size [n_samples, 1]\n",
    "\n",
    "    # support indexing such that dataset[i] can be used to get i-th sample\n",
    "    def __getitem__(self, index):\n",
    "        return self.x_data[index], self.y_data[index]\n",
    "\n",
    "    # we can call len(dataset) to return the size\n",
    "    def __len__(self):\n",
    "        return self.n_samples\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "class my_dataset_test(Dataset):\n",
    "\n",
    "    def __init__(self):\n",
    "        # Initialize data, download, etc.\n",
    "        # read with numpy or pandas\n",
    "        #xy = np.loadtxt('./data/wine/wine.csv', delimiter=',', dtype=np.float32, skiprows=1)\n",
    "\n",
    "        self.n_samples = x_test.shape[0]\n",
    "        \n",
    "        # here the first column is the class label, the rest are the features\n",
    "        self.x_data = torch.tensor(x_test,dtype=torch.float32) # size [n_samples, n_features]\n",
    "        self.y_data = torch.tensor(y_test,dtype=torch.float32) # size [n_samples, 1]\n",
    "\n",
    "    # support indexing such that dataset[i] can be used to get i-th sample\n",
    "    def __getitem__(self, index):\n",
    "        return self.x_data[index], self.y_data[index]\n",
    "\n",
    "    # we can call len(dataset) to return the size\n",
    "    def __len__(self):\n",
    "        return self.n_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wNnQTQcWlDuw"
   },
   "outputs": [],
   "source": [
    "# create dataset\n",
    "dataset_train = my_dataset_train()\n",
    "dataset_test = my_dataset_test()\n",
    "\n",
    "# get first sample and unpack\n",
    "first_data = dataset_train[0]\n",
    "features, labels = first_data\n",
    "print(features, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yGFnWa9alD53"
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Hyper-parameters \n",
    "input_size = x_train.shape[1] # \n",
    "hidden_size1 = 200 \n",
    "hidden_size2 = 100\n",
    "hidden_size3 = 20\n",
    "num_epochs = 1000\n",
    "batch_size = 8\n",
    "learning_rate = 0.0000005\n",
    "\n",
    "\n",
    "\n",
    "# Data loader\n",
    "train_loader = torch.utils.data.DataLoader(dataset=dataset_train, \n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=True)\n",
    "\n",
    "\"\"\"val_loader = torch.utils.data.DataLoader(dataset=dataset_validation, \n",
    "                                          batch_size=batch_size, \n",
    "                                          shuffle=True)\"\"\"\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=dataset_test, \n",
    "                                          batch_size=1, \n",
    "                                          shuffle=False)\n",
    "\n",
    "examples = iter(test_loader)\n",
    "example_data, example_targets = examples.next()\n",
    "\n",
    "\n",
    "\n",
    "# Fully connected neural network with one hidden layer\n",
    "class ConvNet(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size1):\n",
    "        super(ConvNet, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        \"\"\"self.l1 = nn.Linear(input_size, hidden_size1) \n",
    "        self.relu = nn.ReLU()\n",
    "        self.l2 = nn.Linear(hidden_size1, hidden_size2) \n",
    "        self.relu = nn.ReLU() \n",
    "        self.l3 = nn.Linear(hidden_size2, hidden_size3) \n",
    "        self.relu = nn.ReLU()\n",
    "        self.l4 = nn.Linear(hidden_size3, 1)\"\"\"\n",
    "\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv1d(1, 256, kernel_size=5, stride=1, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(kernel_size=2, stride=2))\n",
    "\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv1d(256, 256, kernel_size=5, stride=1, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(kernel_size=2, stride=2))\n",
    "        \n",
    "        self.layer3 = nn.Sequential(\n",
    "            nn.Conv1d(256, 128, kernel_size=5, stride=1, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(kernel_size=2, stride=2))\n",
    "                \n",
    "        self.layer4 = nn.Sequential(\n",
    "            nn.Conv1d(128, 64, kernel_size=5, stride=1, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(kernel_size=2, stride=2))\n",
    "\n",
    "        #self.drop_out = nn.Dropout()\n",
    "        \n",
    "\n",
    "        self.fc1 = nn.Linear(576 , hidden_size1) \n",
    "        self.relu = nn.ReLU() \n",
    "        self.fc2 = nn.Linear(hidden_size1, 3) \n",
    "    \n",
    "    def forward(self, x):\n",
    "        if x.dim() == 3:\n",
    "            x = torch.unsqueeze(x, 0)\n",
    "        x = x.unsqueeze(1)\n",
    "        #print(x.dim())\n",
    "        #print(x.shape)\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "\n",
    "        out = out.reshape(out.size(0), -1)\n",
    "        #print(out.shape)\n",
    "\n",
    "        #out = self.drop_out(out)\n",
    "        out = self.fc1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "\n",
    "\n",
    "\n",
    "        #out = self.l1(x)\n",
    "        \n",
    "        # no activation and no softmax at the end\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Vh-GF4M6lD9d"
   },
   "outputs": [],
   "source": [
    "model = ConvNet(input_size, hidden_size1).to(device)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)#, weight_decay=1e-5)  \n",
    "\n",
    "\n",
    "\n",
    "def my_plot(epochs, loss_train, loss_val):\n",
    "    plt.plot(epochs, loss_train)\n",
    "    plt.plot(epochs,loss_val,'r')\n",
    "# Train the model\n",
    "n_total_steps = len(train_loader)\n",
    "n_total_steps_test = len(test_loader)\n",
    "\n",
    "loss_vals_train =  []\n",
    "loss_vals_val = []\n",
    "\n",
    "for epoch in range(1):\n",
    "    epoch_loss_train = []\n",
    "    epoch_loss_val = []\n",
    "    for i, (audios, labels) in enumerate(train_loader):  \n",
    "        # origin shape: [100, 1, 28, 28]\n",
    "        # resized: [100, 784]\n",
    "        audios = audios.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(audios)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss_train.append(loss.item())\n",
    "        if (i+1) % 500 == 0:\n",
    "            pass\n",
    "            print (f'Epoch [{epoch+1}/{num_epochs}], Train Step [{i+1}/{n_total_steps}], Loss: {loss.item():.4f}')\n",
    "    loss_vals_train.append(sum(epoch_loss_train)/len(epoch_loss_train))\n",
    "\n",
    "    with torch.no_grad():\n",
    "      for i, (audios, labels) in enumerate(test_loader):  \n",
    "          # origin shape: [100, 1, 28, 28]\n",
    "          # resized: [100, 784]\n",
    "          audios = audios.to(device)\n",
    "          labels = labels.to(device)\n",
    "          \n",
    "          # Forward pass\n",
    "          outputs = model(audios)\n",
    "          loss = criterion(outputs, labels)\n",
    "          \n",
    "          # Backward and optimize\n",
    "          \"\"\"optimizer.zero_grad()\n",
    "          loss.backward()\n",
    "          optimizer.step()\"\"\"\n",
    "          \n",
    "          epoch_loss_val.append(loss.item())\n",
    "          if (i+1) % 100 == 0:\n",
    "              pass\n",
    "              print (f'Epoch [{epoch+1}/{num_epochs}], Validation Step [{i+1}/{n_total_steps_test}], Loss: {loss.item():.4f}')\n",
    "    loss_vals_val.append(sum(epoch_loss_val)/len(epoch_loss_val))\n",
    "\n",
    "my_plot(np.linspace(1, num_epochs, num_epochs).astype(int), loss_vals_train, loss_vals_val)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KlPrbUYU_F4P"
   },
   "outputs": [],
   "source": [
    "#print(model_1)\n",
    "model = ConvNet(input_size, 200).to(device)\n",
    "\n",
    "model.load_state_dict(torch.load(dir+ '/Fluency_model',map_location=torch.device('cpu')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f-dLvK7dDQ1I"
   },
   "outputs": [],
   "source": [
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qyn1kSYFlEAd"
   },
   "outputs": [],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "pred_vec = []\n",
    "real_vec = []\n",
    "# since we're not training, we don't need to calculate the gradients for our outputs\n",
    "with torch.no_grad():\n",
    "  for samples, labels in test_loader:\n",
    "      samples = samples.to(device)\n",
    "      \n",
    "      labels = labels.to(device)\n",
    "      outputs = model(samples)\n",
    "\n",
    "      # max returns (value ,index)\n",
    "      _, predicted = torch.max(outputs.data, 1)\n",
    "      _, real = torch.max(labels, 1)\n",
    "\n",
    "      pred_vec.append(predicted.item())\n",
    "      real_vec.append(real.item())\n",
    "\n",
    "      total += labels.size(0)\n",
    "      correct += (predicted == real).sum().item()\n",
    "\n",
    "  print('Accuracy of the network on the test audios: %d %%' % (\n",
    "  100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dGX98X164H5F"
   },
   "outputs": [],
   "source": [
    "#torch.save(model.state_dict(), dir+\"/Fluency_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l6RYixOJu8hc"
   },
   "source": [
    "## Tree based model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Evk8XLgkxUvP"
   },
   "outputs": [],
   "source": [
    "def f(y):\n",
    "  for i in range(3):\n",
    "    if (y[i]==1):\n",
    "      x = i\n",
    "  return x\n",
    "\n",
    "y_test_bis = np.array([f(x) for x in y_test])\n",
    "y_train_bis = np.array([f(x) for x in y_train])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8CfwMYxevlwX"
   },
   "outputs": [],
   "source": [
    "clf = GradientBoostingClassifier(n_estimators=200, learning_rate=0.5, max_depth=3, random_state=0).fit(x_train, y_train_bis)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qk9bMzBRvmmo"
   },
   "outputs": [],
   "source": [
    "print(clf.score(x_test, y_test_bis))\n",
    "print(clf.score(x_train, y_train_bis))\n",
    "print(clf.predict(x_test[2:3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xyNHR7yDvyQ2"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BplmJun6vyxm"
   },
   "source": [
    "## Support Vector Machine Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M4xxlCiGv4Bn"
   },
   "outputs": [],
   "source": [
    "clf = svm.SVC(probability=True)\n",
    "clf.fit(x_train, y_train_bis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bKFFOcRgv31E"
   },
   "outputs": [],
   "source": [
    "clf = pickle_model\n",
    "print(clf.score(x_test, y_test_bis))\n",
    "print(clf.score(x_train, y_train_bis))\n",
    "print(clf.predict(x_test[2:3]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zPLz7X3K2lPr"
   },
   "outputs": [],
   "source": [
    "# Save to file in the current working directory\n",
    "pkl_filename = dir + \"/svc_model_fluency.pkl\"\n",
    "with open(pkl_filename, 'wb') as file:\n",
    "    pickle.dump(clf, file)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hs_YF77b3hTr"
   },
   "outputs": [],
   "source": [
    "# Load from file\n",
    "pkl_filename = dir + \"/svc_model_fluency.pkl\"\n",
    "with open(pkl_filename, 'rb') as file:\n",
    "    pickle_model = pickle.load(file)\n",
    "    \n",
    "# Calculate the accuracy score and predict target values\n",
    "score = pickle_model.score(x_test, y_test_bis)\n",
    "print(\"Test score: {0:.2f} %\".format(100 * score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ArD31ukt1qcn"
   },
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Au9Bhz6gyz_j"
   },
   "outputs": [],
   "source": [
    "def one_hot_bis(vec):\n",
    "  n = len(vec)\n",
    "  l = []\n",
    "  for i in vec:\n",
    "    tmp = np.zeros(3)\n",
    "    tmp[i] = 1\n",
    "    l.append(tmp)\n",
    "  return np.array(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vfsW5CTlySL_"
   },
   "outputs": [],
   "source": [
    "pred_vec = clf.predict(x_test)\n",
    "real_vec = y_test_bis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b-EKLmaXyzX1"
   },
   "outputs": [],
   "source": [
    "#real_vec = f(real_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qJ2lNOfKyhXM"
   },
   "outputs": [],
   "source": [
    "print(real_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ct23tOyNt97y"
   },
   "outputs": [],
   "source": [
    "cm = confusion_matrix(np.array(real_vec), np.array(pred_vec))\n",
    "plt.figure(figsize = (12, 10))\n",
    "cm = pd.DataFrame(cm , index = [i for i in encoder.categories_] , columns = [i for i in encoder.categories_])\n",
    "sns.heatmap(cm, linecolor='white', cmap='Blues', linewidth=1, annot=True, fmt='')\n",
    "plt.title('Confusion Matrix', size=20)\n",
    "plt.xlabel('Predicted Labels', size=14)\n",
    "plt.ylabel('Actual Labels', size=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "So7t3OT_t9lI"
   },
   "outputs": [],
   "source": [
    "meaningful_cols = ['zcr_mean', 'energy_mean', 'energy_entropy_mean', 'spectral_centroid_mean', 'spectral_spread_mean', \n",
    "                   'spectral_entropy_mean', 'spectral_flux_mean', 'spectral_rolloff_mean', 'mfcc_1_mean', 'mfcc_2_mean', \n",
    "                   'mfcc_3_mean', 'mfcc_4_mean', 'mfcc_5_mean', 'mfcc_6_mean', 'mfcc_7_mean', 'mfcc_8_mean',\n",
    "                   'mfcc_9_mean', 'mfcc_10_mean', 'mfcc_11_mean', 'mfcc_12_mean', 'mfcc_13_mean', 'chroma_1_mean',\n",
    "                   'chroma_2_mean', 'chroma_3_mean', 'chroma_4_mean', 'chroma_5_mean', 'chroma_6_mean', 'chroma_7_mean', \n",
    "                   'chroma_8_mean', 'chroma_9_mean', 'chroma_10_mean', 'chroma_11_mean', 'chroma_12_mean',\n",
    "                   'chroma_std_mean', 'delta zcr_mean', 'delta energy_mean', 'delta energy_entropy_mean', \n",
    "                   'delta spectral_centroid_mean', 'delta spectral_spread_mean', 'delta spectral_entropy_mean',\n",
    "                   'delta spectral_flux_mean', 'delta spectral_rolloff_mean', 'delta mfcc_1_mean', 'delta mfcc_2_mean',\n",
    "                   'delta mfcc_3_mean', 'delta mfcc_4_mean', 'delta mfcc_5_mean', 'delta mfcc_6_mean',\n",
    "                   'delta mfcc_7_mean', 'delta mfcc_8_mean', 'delta mfcc_9_mean', 'delta mfcc_10_mean', \n",
    "                   'delta mfcc_11_mean', 'delta mfcc_12_mean', 'delta mfcc_13_mean', 'delta chroma_1_mean',\n",
    "                   'delta chroma_2_mean', 'delta chroma_3_mean', 'delta chroma_4_mean', 'delta chroma_5_mean', \n",
    "                   'delta chroma_6_mean', 'delta chroma_7_mean', 'delta chroma_8_mean', 'delta chroma_9_mean',\n",
    "                   'delta chroma_10_mean', 'delta chroma_11_mean', 'delta chroma_12_mean', 'delta chroma_std_mean',\n",
    "                   'zcr_std', 'energy_std', 'energy_entropy_std', 'spectral_centroid_std', 'spectral_spread_std', \n",
    "                   'spectral_entropy_std', 'spectral_flux_std', 'spectral_rolloff_std', 'mfcc_1_std', 'mfcc_2_std', \n",
    "                   'mfcc_3_std', 'mfcc_4_std', 'mfcc_5_std', 'mfcc_6_std', 'mfcc_7_std', 'mfcc_8_std', 'mfcc_9_std', \n",
    "                   'mfcc_10_std', 'mfcc_11_std', 'mfcc_12_std', 'mfcc_13_std', 'chroma_1_std', 'chroma_2_std', \n",
    "                   'chroma_3_std', 'chroma_4_std', 'chroma_5_std', 'chroma_6_std', 'chroma_7_std', 'chroma_8_std',\n",
    "                   'chroma_9_std', 'chroma_10_std', 'chroma_11_std', 'chroma_12_std', 'chroma_std_std',\n",
    "                   'delta zcr_std', 'delta energy_std', 'delta energy_entropy_std', 'delta spectral_centroid_std',\n",
    "                   'delta spectral_spread_std', 'delta spectral_entropy_std', 'delta spectral_flux_std',\n",
    "                   'delta spectral_rolloff_std', 'delta mfcc_1_std', 'delta mfcc_2_std', 'delta mfcc_3_std',\n",
    "                   'delta mfcc_4_std', 'delta mfcc_5_std', 'delta mfcc_6_std', 'delta mfcc_7_std', \n",
    "                   'delta mfcc_8_std', 'delta mfcc_9_std', 'delta mfcc_10_std', 'delta mfcc_11_std',\n",
    "                   'delta mfcc_12_std', 'delta mfcc_13_std', 'delta chroma_1_std', 'delta chroma_2_std', \n",
    "                   'delta chroma_3_std', 'delta chroma_4_std', 'delta chroma_5_std', 'delta chroma_6_std', \n",
    "                   'delta chroma_7_std', 'delta chroma_8_std', 'delta chroma_9_std', 'delta chroma_10_std', \n",
    "                   'delta chroma_11_std', 'delta chroma_12_std', 'delta chroma_std_std', 'meanF0Hz', 'stdevF0Hz', \n",
    "                   'HNR', 'localJitter', 'localabsoluteJitter', 'rapJitter', 'ppq5Jitter', 'ddpJitter',\n",
    "                   'localShimmer', 'localdbShimmer', 'apq3Shimmer', 'apq5Shimmer', 'apq11Shimmer', 'ddaShimmer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "koMa96rHt8tR"
   },
   "outputs": [],
   "source": [
    "encoder.categories_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "35R-iNvNt8cO"
   },
   "outputs": [],
   "source": [
    "def sentiments_extraction(PATH, model, scaler, meaningful_cols):\n",
    "  audio_df = pd.read_csv(PATH)\n",
    "  audio_df = audio_df[meaningful_cols]\n",
    "  audio_df.dropna(subset = audio_df.columns, inplace=True)\n",
    "\n",
    "  audio_arr = audio_df.values\n",
    "  audio_arr = scaler.transform(audio_arr)\n",
    "  audio_tensor = torch.tensor(audio_arr,dtype=torch.float32)\n",
    "\n",
    "  predictions = []\n",
    "  with torch.no_grad():\n",
    "    for sample in audio_tensor:\n",
    "        #sample = sample.to(device)\n",
    "        #sample = torch.unsqueeze(sample, 0)\n",
    "        #output = model(sample)\n",
    "        output = model.predict(np.array(sample).reshape(1,-1))\n",
    "\n",
    "\n",
    "        #_, predicted = torch.max(output.data, 1)\n",
    "\n",
    "        #predictions.append(predicted.item())\n",
    "        #print(output)\n",
    "        predictions.append(output)\n",
    "\n",
    "  predictions = one_hot_bis(predictions)\n",
    "  predictions = encoder.inverse_transform(predictions)\n",
    "  \n",
    "  return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Uzm1xlHXuRHT"
   },
   "outputs": [],
   "source": [
    "sentiments_p1 = sentiments_extraction(dir+\"/csv_audio_files/P5\", pickle_model, scaler, meaningful_cols)\n",
    "plt.hist(sentiments_p1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6OOtRRoU2fYs"
   },
   "outputs": [],
   "source": [
    "list_of_files = os.listdir(dir+\"/csv_audio_files\")\n",
    "print(list_of_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kOx9OmqA2TvR"
   },
   "outputs": [],
   "source": [
    "def sentiments_percentage_dict(list_of_sentiments):\n",
    "  my_dict = {'low':0, 'intermediate':0, 'high':0}\n",
    "  length = len(list_of_sentiments)\n",
    "  value = 1/length\n",
    "  for sentiment in list_of_sentiments:\n",
    "    #print(sentiment[0])\n",
    "    my_dict[sentiment[0]] += value\n",
    "  \n",
    "  return my_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-MMjaZj02jtT"
   },
   "outputs": [],
   "source": [
    "sentiments_percentage_dict(sentiments_p1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y9wy8vPV2jj8"
   },
   "outputs": [],
   "source": [
    "def sentiments_df (PATH, target_label):\n",
    "  list_of_files = os.listdir(PATH)\n",
    "  n = len(list_of_files)\n",
    "  column_names = [\"Participant\", \"Fluency_percentage\", target_label, \"Length\"]\n",
    "  df = pd.DataFrame(columns = column_names)\n",
    "\n",
    "  labels_df_raw = pd.read_csv(dir+\"/Labels/turker_scores_full_interview.csv\")\n",
    "\n",
    "  for i in range(n):\n",
    "    list_of_sentiments = sentiments_extraction(PATH+\"/\"+list_of_files[i], pickle_model, scaler, meaningful_cols)\n",
    "    sentiments_dict = sentiments_percentage_dict(list_of_sentiments)\n",
    "    length = len(list_of_sentiments)\n",
    "\n",
    "    labels_audio = ['Worker',target_label]#,'NotAwkward','Excited','SpeakingRate','NoFillers','Friendly','EngagingTone','Calm','NotStressed']\n",
    "    bis = labels_df_raw.loc[labels_df_raw[\"Participant\"] == list_of_files[i].lower() ,]\n",
    "    bis = bis.reindex(columns = labels_audio)\n",
    "    score = bis.loc[bis[\"Worker\"] == 'AGGR' ,[target_label]].values[0,0]\n",
    "\n",
    "\n",
    "    df = df.append({\"Participant\":list_of_files[i].lower(), \"Fluency_percentage\":sentiments_dict, target_label:score, \"Length\":length}, ignore_index=True)\n",
    "\n",
    "  return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XCrEJ-kT2jgt"
   },
   "outputs": [],
   "source": [
    "target_label = \"RecommendHiring\"\n",
    "my_df = sentiments_df(dir+\"/csv_audio_files\", target_label)\n",
    "my_df.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ALqRqFve3eJ7"
   },
   "outputs": [],
   "source": [
    "my_df.to_csv(\"overall_audio_fluency_analysis.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w3LBhgPP7GOk"
   },
   "source": [
    "### Graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kSR0AaMe7Irs"
   },
   "outputs": [],
   "source": [
    "#target_label = \"RecommendHiring\"\n",
    "index_good_interviews = my_df[target_label].rank(pct=True) > 0.75\n",
    "index_bad_interviews = my_df[target_label].rank(pct=True) <= 0.25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rKxtsoLo7IpS"
   },
   "outputs": [],
   "source": [
    "def sentiment_aggregate(series):\n",
    "  ['high', 'intermediate', 'low']\n",
    "  high_list = []\n",
    "  intermediate_list = []\n",
    "  low_list = []\n",
    "\n",
    "  out =\t{\n",
    "    'high': high_list,\n",
    "    'intermediate': intermediate_list,\n",
    "    'low': low_list,\n",
    "\n",
    "  }\n",
    "\n",
    "  for dict in series:\n",
    "    high_list.append(dict.get('high'))\n",
    "    intermediate_list.append(dict.get('intermediate'))\n",
    "    low_list.append(dict.get('low'))\n",
    "    \n",
    "  return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Fbmt8o8G7Imt"
   },
   "outputs": [],
   "source": [
    "dict_good_percentile = my_df[index_good_interviews]['Fluency_percentage'].aggregate(sentiment_aggregate)\n",
    "dict_bad_percentile = my_df[index_bad_interviews]['Fluency_percentage'].aggregate(sentiment_aggregate)\n",
    "\n",
    "\n",
    "good_bad_high = pd.concat(axis=0, ignore_index=True, objs=[\n",
    "    pd.DataFrame.from_dict({'value': dict_bad_percentile.get('high'), target_label: 'no'}),\n",
    "    pd.DataFrame.from_dict({'value': dict_good_percentile.get('high'), target_label: 'yes'})\n",
    "])\n",
    "good_bad_intermediate = pd.concat(axis=0, ignore_index=True, objs=[\n",
    "    pd.DataFrame.from_dict({'value': dict_bad_percentile.get('intermediate'), target_label: 'no'}),\n",
    "    pd.DataFrame.from_dict({'value': dict_good_percentile.get('intermediate'), target_label: 'yes'})\n",
    "])\n",
    "good_bad_low = pd.concat(axis=0, ignore_index=True, objs=[\n",
    "    pd.DataFrame.from_dict({'value': dict_bad_percentile.get('low'), target_label: 'no'}),\n",
    "    pd.DataFrame.from_dict({'value': dict_good_percentile.get('low'), target_label: 'yes'})\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cPvcjhjv7IkS"
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(10, 6))\n",
    "sns.kdeplot(data=good_bad_high, hue=target_label, x='value', fill=True, ax=axes[0, 0], common_norm=False).set(title='high')\n",
    "sns.kdeplot(data=good_bad_intermediate, hue=target_label, x='value', fill=True, ax=axes[0, 1], common_norm=False).set(title='intermediate')\n",
    "sns.kdeplot(data=good_bad_low, hue=target_label, x='value', fill=True, ax=axes[1, 0], common_norm=False).set(title='low')\n",
    "plt.delaxes(ax=axes[1, 1])\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XRgajOCO7IMr"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FtptmGoB6GJU"
   },
   "source": [
    "### Correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RXT36d-o6Jvl"
   },
   "outputs": [],
   "source": [
    "corr_list = {}\n",
    "n = my_df.shape[0]\n",
    "for level in ['high', 'intermediate', 'low']:\n",
    "  s_arr = np.zeros(n)\n",
    "  for i in range(n):\n",
    "    s_arr[i] = ( my_df[\"Fluency_percentage\"].values[i][level])\n",
    "  corr_list[level] = s_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KCAYknf76JjM"
   },
   "outputs": [],
   "source": [
    "corr_values = {}\n",
    "for level in ['high', 'intermediate', 'low']:\n",
    "  data1 = corr_list[level]\n",
    "  df2 = my_df['RecommendHiring']\n",
    "  #corr, _ = pearsonr(data1, data2)\n",
    "  df1 = pd.DataFrame(data1,columns=['value'])\n",
    "  #df2 = pd.DataFrame(data2,columns=['RecommendHiring'])\n",
    "  \n",
    "  corr_values[level] = scipy.stats.pearsonr(df2.values, df1.values.reshape(1,-1)[0])#new_df.corr(method ='pearson')['value'].values[1]\n",
    "\n",
    "  #print('Pearsons correlation: %.3f' % corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U1pFGKGb6JBM"
   },
   "outputs": [],
   "source": [
    "corr_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H8Is4P_K41MI"
   },
   "outputs": [],
   "source": [
    "df1.values.reshape(1,-1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z7_jceUX4jRy"
   },
   "outputs": [],
   "source": [
    "import scipy\n",
    "scipy.stats.pearsonr(df2.values, df1.values.reshape(1,-1)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tQQrNoAutUuZ"
   },
   "outputs": [],
   "source": [
    "print(np.percentile(df1[index_good_interviews].values,80))\n",
    "print(np.percentile(df1[index_bad_interviews].values,80))\n",
    "arr = df1.values\n",
    "print(len([x for x in arr if x>0.033]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fwi4vpeMuSU3"
   },
   "source": [
    "# Explanations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Wbw-PBaEu2fy"
   },
   "outputs": [],
   "source": [
    "!pip install shap\n",
    "!pip install lime\n",
    "import lime\n",
    "import lime.lime_tabular\n",
    "import shap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4NGJBeYfum_P"
   },
   "source": [
    "## SHAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "48MES8spFLPL"
   },
   "outputs": [],
   "source": [
    "pkl_filename_shap = dir + \"/svc_model_shap_fluency.pkl\"\n",
    "with open(pkl_filename_shap, 'rb') as file:\n",
    "    pickle_shap = dill.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bh3VEx8eFnhM"
   },
   "outputs": [],
   "source": [
    "tmp_df = pd.DataFrame(x_test, columns=meaningful_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n9RbaYY6MpW9"
   },
   "outputs": [],
   "source": [
    "\"\"\"clf = pickle_model\n",
    "explainer_shap = shap.Explainer(clf.predict,x_train)\n",
    "shap_values = explainer_shap(x_test)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fVQCGmHiFHgo"
   },
   "outputs": [],
   "source": [
    "shap_values = pickle_shap(tmp_df)\n",
    "shap.plots.waterfall(shap_values[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MWwkDS8If09r"
   },
   "outputs": [],
   "source": [
    "shap.summary_plot(shap_values, x_test, feature_names = meaningful_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TDePavC4g2a9"
   },
   "outputs": [],
   "source": [
    "#shap_values = explainer_shap(tmp_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G9CgFzCce_L4"
   },
   "outputs": [],
   "source": [
    "shap.plots.bar(shap_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wMwoK3QIM6o3"
   },
   "outputs": [],
   "source": [
    "# visualize the first prediction's explanation\n",
    "shap.plots.waterfall(shap_values[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TjzIjAi-FOvv"
   },
   "outputs": [],
   "source": [
    "pkl_filename_shap = dir + \"/svc_model_shap_fluency.pkl\"\n",
    "with open(pkl_filename_shap, 'wb') as file:\n",
    "    dill.dump(explainer_shap, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tXx-o0Uvwisb"
   },
   "source": [
    "## LIME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YeHGiGXmM6dJ"
   },
   "outputs": [],
   "source": [
    "explainer_lime = lime.lime_tabular.LimeTabularExplainer(x_train, feature_names=meaningful_cols, class_names=['high', 'intermediate', 'low'], verbose=True, mode='classification')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7Ie2n2pmM6aB"
   },
   "outputs": [],
   "source": [
    "exp = explainer_lime.explain_instance(x_test[2], clf.predict_proba, num_features=5)\n",
    "exp.show_in_notebook(show_table=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4b7PbzgyaHhC"
   },
   "outputs": [],
   "source": [
    "clf.probability = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zKQtUlJnbSGZ"
   },
   "outputs": [],
   "source": [
    "pkl_filename_lime = dir + \"/svc_model_lime_fluency.pkl\"\n",
    "with open(pkl_filename_lime, 'wb') as file:\n",
    "    dill.dump(explainer_lime, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VPB6xiv0FiN3"
   },
   "outputs": [],
   "source": [
    "# Load from file\n",
    "pkl_filename_lime = dir + \"/svc_model_lime_fluency.pkl\"\n",
    "with open(pkl_filename_lime, 'rb') as file:\n",
    "    pickle_lime = dill.load(file)\n",
    "    \n",
    "exp = pickle_lime.explain_instance(x_test[3], clf.predict_proba, num_features=5, top_labels=3)\n",
    "exp.show_in_notebook(show_table=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oHBJ_P7aFwHV"
   },
   "outputs": [],
   "source": [
    "import dill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "04-C95KIGI3x"
   },
   "outputs": [],
   "source": [
    "mdf = pd.read_csv(\"testing_Ex.csv\",index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m_yDC4CGHInS"
   },
   "outputs": [],
   "source": [
    "x = mdf.values[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AqYSP6wtVuNT"
   },
   "outputs": [],
   "source": [
    "x.show_in_notebook(show_table=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w-e1KBzHZ5uT"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lC67K3BCV71d"
   },
   "outputs": [],
   "source": [
    "exp = pickle_lime.explain_instance(x_test[3], clf.predict_proba, num_features=5)\n",
    "exp.show_in_notebook(show_table=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kqXh3whfc800"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "fluency_classification.ipynb",
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RG7EHYuTHvyN"
   },
   "source": [
    "# Importing packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "az7GZszAlBuc"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import os\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor,RandomForestClassifier\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "import math\n",
    "\n",
    "import dill\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn import svm\n",
    "import pickle\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.lines as mlines\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# to play the audio files\n",
    "from IPython.display import Audio\n",
    "\n",
    "# Neural networks can be constructed using the torch.nn package.\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torchvision import models,transforms,datasets\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qZvuqjjTlCOZ"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LRDZdqlEt9nj"
   },
   "source": [
    "# Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5rZ-VMOblCZO"
   },
   "outputs": [],
   "source": [
    "dir = \"drive/MyDrive/Colab_Notebooks/mit_data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mRtD8-v-lCjz"
   },
   "outputs": [],
   "source": [
    "dataset_df = pd.read_csv(dir+\"/features_sentiments.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2gKbBdPklCtm"
   },
   "outputs": [],
   "source": [
    "dataset_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fOBx3QdTlC_g"
   },
   "outputs": [],
   "source": [
    "columns_with_nan_values = []\n",
    "for clmn in dataset_df.columns:\n",
    "  num = len(dataset_df[clmn].values[dataset_df[clmn].isnull()])\n",
    "  if (num !=0):\n",
    "    columns_with_nan_values.append(clmn)\n",
    "print(len(columns_with_nan_values))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ENwMUkXMlDFp"
   },
   "outputs": [],
   "source": [
    "dataset_df.dropna(subset = columns_with_nan_values, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LDnITLjdlDKj"
   },
   "outputs": [],
   "source": [
    "columns_with_nan_values = []\n",
    "for clmn in dataset_df.columns:\n",
    "  num = len(dataset_df[clmn].values[dataset_df[clmn].isnull()])\n",
    "  if (num !=0):\n",
    "    columns_with_nan_values.append(clmn)\n",
    "print(len(columns_with_nan_values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QJRpzSOUvJ_g"
   },
   "outputs": [],
   "source": [
    "X = dataset_df.iloc[: ,:-1].values\n",
    "Y = dataset_df['labels'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "buCphpLeuXoU"
   },
   "outputs": [],
   "source": [
    "print(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kszZ1GFOvJws"
   },
   "outputs": [],
   "source": [
    "# As this is a multiclass classification problem onehotencoding our Y.\n",
    "encoder = OneHotEncoder()\n",
    "Y = encoder.fit_transform(np.array(Y).reshape(-1,1)).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "48dFFLINvW3q"
   },
   "outputs": [],
   "source": [
    "print(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ni7VpxGlvdY0"
   },
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, random_state=0, shuffle=True)\n",
    "x_train.shape, y_train.shape, x_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7umkKq0mvJQ1"
   },
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "x_train = scaler.fit_transform(x_train)\n",
    "x_test = scaler.transform(x_test)\n",
    "x_train.shape, y_train.shape, x_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zvC5W5D3wRyl"
   },
   "source": [
    "# Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eMw-OevexUNG"
   },
   "source": [
    "## CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1TMs1oROlDaG"
   },
   "outputs": [],
   "source": [
    "x_train_tensor = torch.tensor(x_train,dtype=torch.float32)\n",
    "x_test_tensor = torch.tensor(x_test,dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train,dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test,dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Gg_XPSktlDoe"
   },
   "outputs": [],
   "source": [
    "class my_dataset_train(Dataset):\n",
    "\n",
    "    def __init__(self):\n",
    "        # Initialize data, download, etc.\n",
    "        # read with numpy or pandas\n",
    "        #xy = np.loadtxt('./data/wine/wine.csv', delimiter=',', dtype=np.float32, skiprows=1)\n",
    "\n",
    "        self.n_samples = x_train.shape[0]\n",
    "        \n",
    "        # here the first column is the class label, the rest are the features\n",
    "        self.x_data = torch.tensor(x_train,dtype=torch.float32) # size [n_samples, n_features]\n",
    "        self.y_data = torch.tensor(y_train,dtype=torch.float32) # size [n_samples, 1]\n",
    "\n",
    "    # support indexing such that dataset[i] can be used to get i-th sample\n",
    "    def __getitem__(self, index):\n",
    "        return self.x_data[index], self.y_data[index]\n",
    "\n",
    "    # we can call len(dataset) to return the size\n",
    "    def __len__(self):\n",
    "        return self.n_samples\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "class my_dataset_test(Dataset):\n",
    "\n",
    "    def __init__(self):\n",
    "        # Initialize data, download, etc.\n",
    "        # read with numpy or pandas\n",
    "        #xy = np.loadtxt('./data/wine/wine.csv', delimiter=',', dtype=np.float32, skiprows=1)\n",
    "\n",
    "        self.n_samples = x_test.shape[0]\n",
    "        \n",
    "        # here the first column is the class label, the rest are the features\n",
    "        self.x_data = torch.tensor(x_test,dtype=torch.float32) # size [n_samples, n_features]\n",
    "        self.y_data = torch.tensor(y_test,dtype=torch.float32) # size [n_samples, 1]\n",
    "\n",
    "    # support indexing such that dataset[i] can be used to get i-th sample\n",
    "    def __getitem__(self, index):\n",
    "        return self.x_data[index], self.y_data[index]\n",
    "\n",
    "    # we can call len(dataset) to return the size\n",
    "    def __len__(self):\n",
    "        return self.n_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wNnQTQcWlDuw"
   },
   "outputs": [],
   "source": [
    "# create dataset\n",
    "dataset_train = my_dataset_train()\n",
    "dataset_test = my_dataset_test()\n",
    "\n",
    "# get first sample and unpack\n",
    "first_data = dataset_train[0]\n",
    "features, labels = first_data\n",
    "print(features, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yGFnWa9alD53"
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Hyper-parameters \n",
    "input_size = x_train.shape[1] # \n",
    "hidden_size1 = 200 \n",
    "hidden_size2 = 100\n",
    "hidden_size3 = 20\n",
    "num_epochs = 300\n",
    "batch_size = 16\n",
    "learning_rate = 0.000001\n",
    "\n",
    "\n",
    "\n",
    "# Data loader\n",
    "train_loader = torch.utils.data.DataLoader(dataset=dataset_train, \n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=True)\n",
    "\n",
    "\"\"\"val_loader = torch.utils.data.DataLoader(dataset=dataset_validation, \n",
    "                                          batch_size=batch_size, \n",
    "                                          shuffle=True)\"\"\"\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=dataset_test, \n",
    "                                          batch_size=1, \n",
    "                                          shuffle=False)\n",
    "\n",
    "examples = iter(test_loader)\n",
    "example_data, example_targets = examples.next()\n",
    "\n",
    "\n",
    "\n",
    "# Fully connected neural network with one hidden layer\n",
    "class ConvNet(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size1):\n",
    "        super(ConvNet, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        \"\"\"self.l1 = nn.Linear(input_size, hidden_size1) \n",
    "        self.relu = nn.ReLU()\n",
    "        self.l2 = nn.Linear(hidden_size1, hidden_size2) \n",
    "        self.relu = nn.ReLU() \n",
    "        self.l3 = nn.Linear(hidden_size2, hidden_size3) \n",
    "        self.relu = nn.ReLU()\n",
    "        self.l4 = nn.Linear(hidden_size3, 1)\"\"\"\n",
    "\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv1d(1, 256, kernel_size=5, stride=1, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(kernel_size=2, stride=2))\n",
    "\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv1d(256, 256, kernel_size=5, stride=1, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(kernel_size=2, stride=2))\n",
    "        \n",
    "        self.layer3 = nn.Sequential(\n",
    "            nn.Conv1d(256, 128, kernel_size=5, stride=1, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(kernel_size=2, stride=2))\n",
    "                \n",
    "        self.layer4 = nn.Sequential(\n",
    "            nn.Conv1d(128, 64, kernel_size=5, stride=1, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(kernel_size=2, stride=2))\n",
    "\n",
    "        #self.drop_out = nn.Dropout()\n",
    "        \n",
    "\n",
    "        self.fc1 = nn.Linear(576 , hidden_size1) \n",
    "        self.relu = nn.ReLU() \n",
    "        self.fc2 = nn.Linear(hidden_size1, 7) \n",
    "    \n",
    "    def forward(self, x):\n",
    "        if x.dim() == 3:\n",
    "            x = torch.unsqueeze(x, 0)\n",
    "        x = x.unsqueeze(1)\n",
    "        #print(x.dim())\n",
    "        #print(x.shape)\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "\n",
    "        out = out.reshape(out.size(0), -1)\n",
    "        #print(out.shape)\n",
    "\n",
    "        #out = self.drop_out(out)\n",
    "        out = self.fc1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "\n",
    "\n",
    "\n",
    "        #out = self.l1(x)\n",
    "        \n",
    "        # no activation and no softmax at the end\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Vh-GF4M6lD9d"
   },
   "outputs": [],
   "source": [
    "model = ConvNet(input_size, hidden_size1).to(device)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)#, weight_decay=1e-5)  \n",
    "\n",
    "\n",
    "\n",
    "def my_plot(epochs, loss_train, loss_val):\n",
    "    plt.plot(epochs, loss_train)\n",
    "    plt.plot(epochs,loss_val,'r')\n",
    "# Train the model\n",
    "n_total_steps = len(train_loader)\n",
    "n_total_steps_test = len(test_loader)\n",
    "\n",
    "loss_vals_train =  []\n",
    "loss_vals_val = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_loss_train = []\n",
    "    epoch_loss_val = []\n",
    "    for i, (audios, labels) in enumerate(train_loader):  \n",
    "        # origin shape: [100, 1, 28, 28]\n",
    "        # resized: [100, 784]\n",
    "        audios = audios.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(audios)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss_train.append(loss.item())\n",
    "        if (i+1) % 500 == 0:\n",
    "            pass\n",
    "            print (f'Epoch [{epoch+1}/{num_epochs}], Train Step [{i+1}/{n_total_steps}], Loss: {loss.item():.4f}')\n",
    "    loss_vals_train.append(sum(epoch_loss_train)/len(epoch_loss_train))\n",
    "\n",
    "    with torch.no_grad():\n",
    "      for i, (audios, labels) in enumerate(test_loader):  \n",
    "          # origin shape: [100, 1, 28, 28]\n",
    "          # resized: [100, 784]\n",
    "          audios = audios.to(device)\n",
    "          labels = labels.to(device)\n",
    "          \n",
    "          # Forward pass\n",
    "          outputs = model(audios)\n",
    "          loss = criterion(outputs, labels)\n",
    "          \n",
    "          # Backward and optimize\n",
    "          \"\"\"optimizer.zero_grad()\n",
    "          loss.backward()\n",
    "          optimizer.step()\"\"\"\n",
    "          \n",
    "          epoch_loss_val.append(loss.item())\n",
    "          if (i+1) % 100 == 0:\n",
    "              pass\n",
    "              print (f'Epoch [{epoch+1}/{num_epochs}], Validation Step [{i+1}/{n_total_steps_test}], Loss: {loss.item():.4f}')\n",
    "    loss_vals_val.append(sum(epoch_loss_val)/len(epoch_loss_val))\n",
    "\n",
    "my_plot(np.linspace(1, num_epochs, num_epochs).astype(int), loss_vals_train, loss_vals_val)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KlPrbUYU_F4P"
   },
   "outputs": [],
   "source": [
    "#print(model_1)\n",
    "my_model = ConvNet(input_size, 200).to(device)\n",
    "\n",
    "my_model.load_state_dict(torch.load(dir+ '/sentiment_classification_model'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f-dLvK7dDQ1I"
   },
   "outputs": [],
   "source": [
    "my_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qyn1kSYFlEAd"
   },
   "outputs": [],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "pred_vec = []\n",
    "real_vec = []\n",
    "# since we're not training, we don't need to calculate the gradients for our outputs\n",
    "with torch.no_grad():\n",
    "  for samples, labels in test_loader:\n",
    "      samples = samples.to(device)\n",
    "      \n",
    "      labels = labels.to(device)\n",
    "      outputs = my_model(samples)\n",
    "\n",
    "      # max returns (value ,index)\n",
    "      _, predicted = torch.max(outputs.data, 1)\n",
    "      _, real = torch.max(labels, 1)\n",
    "\n",
    "      pred_vec.append(predicted.item())\n",
    "      real_vec.append(real.item())\n",
    "\n",
    "      total += labels.size(0)\n",
    "      correct += (predicted == real).sum().item()\n",
    "\n",
    "  print('Accuracy of the network on the test audios: %d %%' % (\n",
    "  100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dGX98X164H5F"
   },
   "outputs": [],
   "source": [
    "#torch.save(model.state_dict(), dir+\"/my_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SMMVbt7V8rZA"
   },
   "source": [
    "## Tree based model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LNdafdzk8sJ3"
   },
   "outputs": [],
   "source": [
    "def f(y):\n",
    "  x=0\n",
    "  for i in range(7):\n",
    "    if (y[i]==1):\n",
    "      x = i\n",
    "  return x\n",
    "  \n",
    "y_test_bis = np.array([f(x) for x in y_test])\n",
    "y_train_bis = np.array([f(x) for x in y_train])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "90WlNs-X8sws"
   },
   "outputs": [],
   "source": [
    "clf = GradientBoostingClassifier(n_estimators=100, learning_rate=0.5, max_depth=2, random_state=0).fit(x_train, y_train_bis)\n",
    "clf.score(x_test, y_test_bis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I7lWUkYK2OpA"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JTajMRaO2Omj"
   },
   "outputs": [],
   "source": [
    "rf_model = RandomForestClassifier(n_estimators=800, max_depth=2, random_state=0)\n",
    "rf_model.fit(x_train, y_train_bis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NuNzBwmc2OCY"
   },
   "outputs": [],
   "source": [
    "print(rf_model.score(x_test, y_test_bis))\n",
    "print(rf_model.score(x_train, y_train_bis))\n",
    "print(rf_model.predict(x_test[2:3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FooOCYuo8stJ"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zz4s9DOMHgtz"
   },
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uechLzXW8spw"
   },
   "outputs": [],
   "source": [
    "svm_model = svm.SVC(kernel='rbf',C=1.1, probability=True)\n",
    "svm_model.fit(x_train, y_train_bis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Iq8WocdR8sgR"
   },
   "outputs": [],
   "source": [
    "print(svm_model.score(x_test, y_test_bis))\n",
    "print(svm_model.score(x_train, y_train_bis))\n",
    "print(svm_model.predict(x_test[2:3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QSDrO_fg2eDR"
   },
   "outputs": [],
   "source": [
    "clf = pickle_model\n",
    "pred_vec = clf.predict(x_test)\n",
    "real_vec = y_test_bis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4M0NY1Yt4uyn"
   },
   "outputs": [],
   "source": [
    "# Save to file in the current working directory\n",
    "pkl_filename = dir + \"/svm_sentiment_model.pkl\"\n",
    "with open(pkl_filename, 'wb') as file:\n",
    "    pickle.dump(clf, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8hpNY8qC46O2"
   },
   "outputs": [],
   "source": [
    "# Load from file\n",
    "pkl_filename = dir + \"/svm_sentiment_model.pkl\"\n",
    "with open(pkl_filename, 'rb') as file:\n",
    "    pickle_model = pickle.load(file)\n",
    "    \n",
    "# Calculate the accuracy score and predict target values\n",
    "score = pickle_model.score(x_test, y_test_bis)\n",
    "print(\"Test score: {0:.2f} %\".format(100 * score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iuKvUb6q8dqL"
   },
   "source": [
    "# Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6rJDxN9LlEdy"
   },
   "outputs": [],
   "source": [
    "cm = confusion_matrix(np.array(real_vec), np.array(pred_vec))\n",
    "plt.figure(figsize = (12, 10))\n",
    "cm = pd.DataFrame(cm , index = [i for i in encoder.categories_] , columns = [i for i in encoder.categories_])\n",
    "sns.heatmap(cm, linecolor='white', cmap='Blues', linewidth=1, annot=True, fmt='')\n",
    "plt.title('Confusion Matrix', size=20)\n",
    "plt.xlabel('Predicted Labels', size=14)\n",
    "plt.ylabel('Actual Labels', size=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eijfcJOZR24k"
   },
   "outputs": [],
   "source": [
    "meaningful_cols = ['zcr_mean', 'energy_mean', 'energy_entropy_mean', 'spectral_centroid_mean', 'spectral_spread_mean', \n",
    "                   'spectral_entropy_mean', 'spectral_flux_mean', 'spectral_rolloff_mean', 'mfcc_1_mean', 'mfcc_2_mean', \n",
    "                   'mfcc_3_mean', 'mfcc_4_mean', 'mfcc_5_mean', 'mfcc_6_mean', 'mfcc_7_mean', 'mfcc_8_mean',\n",
    "                   'mfcc_9_mean', 'mfcc_10_mean', 'mfcc_11_mean', 'mfcc_12_mean', 'mfcc_13_mean', 'chroma_1_mean',\n",
    "                   'chroma_2_mean', 'chroma_3_mean', 'chroma_4_mean', 'chroma_5_mean', 'chroma_6_mean', 'chroma_7_mean', \n",
    "                   'chroma_8_mean', 'chroma_9_mean', 'chroma_10_mean', 'chroma_11_mean', 'chroma_12_mean',\n",
    "                   'chroma_std_mean', 'delta zcr_mean', 'delta energy_mean', 'delta energy_entropy_mean', \n",
    "                   'delta spectral_centroid_mean', 'delta spectral_spread_mean', 'delta spectral_entropy_mean',\n",
    "                   'delta spectral_flux_mean', 'delta spectral_rolloff_mean', 'delta mfcc_1_mean', 'delta mfcc_2_mean',\n",
    "                   'delta mfcc_3_mean', 'delta mfcc_4_mean', 'delta mfcc_5_mean', 'delta mfcc_6_mean',\n",
    "                   'delta mfcc_7_mean', 'delta mfcc_8_mean', 'delta mfcc_9_mean', 'delta mfcc_10_mean', \n",
    "                   'delta mfcc_11_mean', 'delta mfcc_12_mean', 'delta mfcc_13_mean', 'delta chroma_1_mean',\n",
    "                   'delta chroma_2_mean', 'delta chroma_3_mean', 'delta chroma_4_mean', 'delta chroma_5_mean', \n",
    "                   'delta chroma_6_mean', 'delta chroma_7_mean', 'delta chroma_8_mean', 'delta chroma_9_mean',\n",
    "                   'delta chroma_10_mean', 'delta chroma_11_mean', 'delta chroma_12_mean', 'delta chroma_std_mean',\n",
    "                   'zcr_std', 'energy_std', 'energy_entropy_std', 'spectral_centroid_std', 'spectral_spread_std', \n",
    "                   'spectral_entropy_std', 'spectral_flux_std', 'spectral_rolloff_std', 'mfcc_1_std', 'mfcc_2_std', \n",
    "                   'mfcc_3_std', 'mfcc_4_std', 'mfcc_5_std', 'mfcc_6_std', 'mfcc_7_std', 'mfcc_8_std', 'mfcc_9_std', \n",
    "                   'mfcc_10_std', 'mfcc_11_std', 'mfcc_12_std', 'mfcc_13_std', 'chroma_1_std', 'chroma_2_std', \n",
    "                   'chroma_3_std', 'chroma_4_std', 'chroma_5_std', 'chroma_6_std', 'chroma_7_std', 'chroma_8_std',\n",
    "                   'chroma_9_std', 'chroma_10_std', 'chroma_11_std', 'chroma_12_std', 'chroma_std_std',\n",
    "                   'delta zcr_std', 'delta energy_std', 'delta energy_entropy_std', 'delta spectral_centroid_std',\n",
    "                   'delta spectral_spread_std', 'delta spectral_entropy_std', 'delta spectral_flux_std',\n",
    "                   'delta spectral_rolloff_std', 'delta mfcc_1_std', 'delta mfcc_2_std', 'delta mfcc_3_std',\n",
    "                   'delta mfcc_4_std', 'delta mfcc_5_std', 'delta mfcc_6_std', 'delta mfcc_7_std', \n",
    "                   'delta mfcc_8_std', 'delta mfcc_9_std', 'delta mfcc_10_std', 'delta mfcc_11_std',\n",
    "                   'delta mfcc_12_std', 'delta mfcc_13_std', 'delta chroma_1_std', 'delta chroma_2_std', \n",
    "                   'delta chroma_3_std', 'delta chroma_4_std', 'delta chroma_5_std', 'delta chroma_6_std', \n",
    "                   'delta chroma_7_std', 'delta chroma_8_std', 'delta chroma_9_std', 'delta chroma_10_std', \n",
    "                   'delta chroma_11_std', 'delta chroma_12_std', 'delta chroma_std_std', 'meanF0Hz', 'stdevF0Hz', \n",
    "                   'HNR', 'localJitter', 'localabsoluteJitter', 'rapJitter', 'ppq5Jitter', 'ddpJitter',\n",
    "                   'localShimmer', 'localdbShimmer', 'apq3Shimmer', 'apq5Shimmer', 'apq11Shimmer', 'ddaShimmer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-OYXSBgqS8JL"
   },
   "outputs": [],
   "source": [
    "def one_hot_bis(vec):\n",
    "  n = len(vec)\n",
    "  l = []\n",
    "  for i in vec:\n",
    "    tmp = np.zeros(7)\n",
    "    tmp[i] = 1\n",
    "    l.append(tmp)\n",
    "  return np.array(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GoSte8ZHU3TI"
   },
   "outputs": [],
   "source": [
    "encoder.categories_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m0SEyKRex2tx"
   },
   "outputs": [],
   "source": [
    "def sentiments_extraction(PATH, model, scaler, meaningful_cols):\n",
    "  audio_df = pd.read_csv(PATH)\n",
    "  audio_df = audio_df[meaningful_cols]\n",
    "  audio_df.dropna(subset = audio_df.columns, inplace=True)\n",
    "\n",
    "  audio_arr = audio_df.values\n",
    "  audio_arr = scaler.transform(audio_arr)\n",
    "  audio_tensor = torch.tensor(audio_arr,dtype=torch.float32)\n",
    "\n",
    "  predictions = []\n",
    "  with torch.no_grad():\n",
    "    for sample in audio_tensor:\n",
    "        #sample = sample.to(device)\n",
    "        #sample = torch.unsqueeze(sample, 0)\n",
    "        #output = my_model(sample)\n",
    "\n",
    "        output = model.predict(np.array(sample).reshape(1,-1))\n",
    "\n",
    "        predictions.append(output)\n",
    "        #_, predicted = torch.max(output.data, 1)\n",
    "\n",
    "        #predictions.append(predicted.item())\n",
    "\n",
    "  predictions = one_hot_bis(predictions)\n",
    "  predictions = encoder.inverse_transform(predictions)\n",
    "  \n",
    "  return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JBIv_-Iiz_Ok"
   },
   "outputs": [],
   "source": [
    "sentiments_p1 = sentiments_extraction(dir+\"/csv_audio_files/PP89\", pickle_model, scaler, meaningful_cols)\n",
    "plt.hist(sentiments_p1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5zStnfgO0J_H"
   },
   "outputs": [],
   "source": [
    "list_of_files = os.listdir(dir+\"/csv_audio_files\")\n",
    "print(list_of_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XojPXrWcAoxF"
   },
   "outputs": [],
   "source": [
    "def sentiments_percentage_dict(list_of_sentiments):\n",
    "  my_dict = {'angry':0, 'calm':0, 'disgust':0, 'fear':0, 'happy':0, 'sad':0, 'surprise':0}\n",
    "  length = len(list_of_sentiments)\n",
    "  value = 1/length\n",
    "  for sentiment in list_of_sentiments:\n",
    "    my_dict[sentiment[0]] += value\n",
    "  \n",
    "  return my_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cyFYMv_GBdOY"
   },
   "outputs": [],
   "source": [
    "sentiments_percentage_dict(sentiments_p1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C6f91IG9-7NQ"
   },
   "outputs": [],
   "source": [
    "def sentiments_df (PATH, target_label):\n",
    "  list_of_files = os.listdir(PATH)\n",
    "  n = len(list_of_files)\n",
    "  column_names = [\"Participant\", \"Sentiments_percentage\", target_label, \"Length\"]\n",
    "  df = pd.DataFrame(columns = column_names)\n",
    "\n",
    "  labels_df_raw = pd.read_csv(dir+\"/Labels/turker_scores_full_interview.csv\")\n",
    "\n",
    "  for i in range(n):\n",
    "    list_of_sentiments = sentiments_extraction(PATH+\"/\"+list_of_files[i], pickle_model, scaler, meaningful_cols)\n",
    "    sentiments_dict = sentiments_percentage_dict(list_of_sentiments)\n",
    "    length = len(list_of_sentiments)\n",
    "\n",
    "    labels_audio = ['Worker',target_label]#,'NotAwkward','Excited','SpeakingRate','NoFillers','Friendly','EngagingTone','Calm','NotStressed']\n",
    "    bis = labels_df_raw.loc[labels_df_raw[\"Participant\"] == list_of_files[i].lower() ,]\n",
    "    bis = bis.reindex(columns = labels_audio)\n",
    "    score = bis.loc[bis[\"Worker\"] == 'AGGR' ,[target_label]].values[0,0]\n",
    "\n",
    "\n",
    "    df = df.append({\"Participant\":list_of_files[i].lower(), \"Sentiments_percentage\":sentiments_dict, target_label:score, \"Length\":length}, ignore_index=True)\n",
    "\n",
    "  return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5HPQHc74Jeie"
   },
   "outputs": [],
   "source": [
    "target_label = \"RecommendHiring\"\n",
    "my_df = sentiments_df(dir+\"/csv_audio_files\", target_label)\n",
    "my_df.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Nc_HLD2iyLW2"
   },
   "outputs": [],
   "source": [
    "my_df.to_csv(\"overall_audio_sentiment_analysis.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "00DFUnfE7hxF"
   },
   "source": [
    "### Graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j2g7TbrbvmGU"
   },
   "outputs": [],
   "source": [
    "def stdev_lines(emotion):\n",
    "  return [mean_stdev.loc[emotion, 'mean'] + mean_stdev.loc[emotion, 'stdev'], mean_stdev.loc[emotion, 'mean'] - mean_stdev.loc[emotion, 'stdev']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MNP5nIRXvlpZ"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P4YG3QyZISX4"
   },
   "outputs": [],
   "source": [
    "#target_label = \"RecommendHiring\"\n",
    "index_good_interviews = my_df[target_label].rank(pct=True) > 0.75\n",
    "index_bad_interviews = my_df[target_label].rank(pct=True) <= 0.25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x8EJM6SY-6v7"
   },
   "outputs": [],
   "source": [
    "def sentiment_aggregate(series):\n",
    "  ['angry', 'calm', 'disgust', 'fear', 'happy', 'sad', 'surprise']\n",
    "  angry_list = []\n",
    "  calm_list = []\n",
    "  disgust_list = []\n",
    "  fear_list = []\n",
    "  happy_list = []\n",
    "  sad_list = []\n",
    "  surprise_list = []\n",
    "\n",
    "  out =\t{\n",
    "    'angry': angry_list,\n",
    "    'calm': calm_list,\n",
    "    'disgust': disgust_list,\n",
    "    'fear':fear_list,\n",
    "    'happy': happy_list,\n",
    "    'sad': sad_list,\n",
    "    'surprise': surprise_list\n",
    "  }\n",
    "\n",
    "  for dict in series:\n",
    "    angry_list.append(dict.get('angry'))\n",
    "    calm_list.append(dict.get('calm'))\n",
    "    disgust_list.append(dict.get('disgust'))\n",
    "    fear_list.append(dict.get('fear'))\n",
    "    happy_list.append(dict.get('happy'))\n",
    "    sad_list.append(dict.get('sad'))\n",
    "    surprise_list.append(dict.get('surprise'))\n",
    "    \n",
    "  return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JF4PajUPN3rn"
   },
   "outputs": [],
   "source": [
    "dict_good_percentile = my_df[index_good_interviews]['Sentiments_percentage'].aggregate(sentiment_aggregate)\n",
    "dict_bad_percentile = my_df[index_bad_interviews]['Sentiments_percentage'].aggregate(sentiment_aggregate)\n",
    "\n",
    "\n",
    "good_bad_angry = pd.concat(axis=0, ignore_index=True, objs=[\n",
    "    pd.DataFrame.from_dict({'value': dict_bad_percentile.get('angry'), target_label: 'no'}),\n",
    "    pd.DataFrame.from_dict({'value': dict_good_percentile.get('angry'), target_label: 'yes'})\n",
    "])\n",
    "good_bad_calm = pd.concat(axis=0, ignore_index=True, objs=[\n",
    "    pd.DataFrame.from_dict({'value': dict_bad_percentile.get('calm'), target_label: 'no'}),\n",
    "    pd.DataFrame.from_dict({'value': dict_good_percentile.get('calm'), target_label: 'yes'})\n",
    "])\n",
    "good_bad_disgust = pd.concat(axis=0, ignore_index=True, objs=[\n",
    "    pd.DataFrame.from_dict({'value': dict_bad_percentile.get('disgust'), target_label: 'no'}),\n",
    "    pd.DataFrame.from_dict({'value': dict_good_percentile.get('disgust'), target_label: 'yes'})\n",
    "])\n",
    "good_bad_fear = pd.concat(axis=0, ignore_index=True, objs=[\n",
    "    pd.DataFrame.from_dict({'value': dict_bad_percentile.get('fear'), target_label: 'no'}),\n",
    "    pd.DataFrame.from_dict({'value': dict_good_percentile.get('fear'), target_label: 'yes'})\n",
    "])\n",
    "good_bad_happy = pd.concat(axis=0, ignore_index=True, objs=[\n",
    "    pd.DataFrame.from_dict({'value': dict_bad_percentile.get('happy'), target_label: 'no'}),\n",
    "    pd.DataFrame.from_dict({'value': dict_good_percentile.get('happy'), target_label: 'yes'})\n",
    "])\n",
    "good_bad_sad = pd.concat(axis=0, ignore_index=True, objs=[\n",
    "    pd.DataFrame.from_dict({'value': dict_bad_percentile.get('sad'), target_label: 'no'}),\n",
    "    pd.DataFrame.from_dict({'value': dict_good_percentile.get('sad'), target_label: 'yes'})\n",
    "])\n",
    "good_bad_surprise = pd.concat(axis=0, ignore_index=True, objs=[\n",
    "    pd.DataFrame.from_dict({'value': dict_bad_percentile.get('surprise'), target_label: 'no'}),\n",
    "    pd.DataFrame.from_dict({'value': dict_good_percentile.get('surprise'), target_label: 'yes'})\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IWc7lhEAN3ks"
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(4, 2, figsize=(15, 9))\n",
    "sns.kdeplot(data=good_bad_angry, hue=target_label, x='value', fill=True, ax=axes[0, 0], common_norm=False).set(title='angry')\n",
    "sns.kdeplot(data=good_bad_calm, hue=target_label, x='value', fill=True, ax=axes[0, 1], common_norm=False).set(title='calm')\n",
    "sns.kdeplot(data=good_bad_disgust, hue=target_label, x='value', fill=True, ax=axes[1, 0], common_norm=False).set(title='disgust')\n",
    "sns.kdeplot(data=good_bad_fear, hue=target_label, x='value', fill=True, ax=axes[1, 1], common_norm=False).set(title='fear')\n",
    "sns.kdeplot(data=good_bad_happy, hue=target_label, x='value', fill=True, ax=axes[2, 0], common_norm=False).set(title='happy')\n",
    "sns.kdeplot(data=good_bad_sad, hue=target_label, x='value', fill=True, ax=axes[2, 1], common_norm=False).set(title='sad')\n",
    "sns.kdeplot(data=good_bad_surprise, hue=target_label, x='value', fill=True, ax=axes[3, 0], common_norm=False).set(title='surprise')\n",
    "plt.delaxes(ax=axes[3, 1])\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T8exNAWCAwgO"
   },
   "outputs": [],
   "source": [
    "df_std.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OlqYeAr12xJP"
   },
   "outputs": [],
   "source": [
    "import ast\n",
    "df_std = my_df.copy()\n",
    "#df_std['Sentiments_percentage'] = df_std['Sentiments_percentage'].apply(ast.literal_eval)\n",
    "df_std = df_std.join(df_std['Sentiments_percentage'].apply(pd.Series))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kQyLIVLgwx5x"
   },
   "outputs": [],
   "source": [
    "import statistics\n",
    "mean = df_std[index_good_interviews][['angry', 'calm', 'disgust', 'fear', 'happy', 'sad', 'surprise']].aggregate(statistics.mean)\n",
    "stdev = df_std[index_good_interviews][['angry', 'calm', 'disgust', 'fear', 'happy', 'sad', 'surprise']].aggregate(statistics.stdev)\n",
    "\n",
    "mean_stdev = pd.concat({'mean': mean, 'stdev': stdev}, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WKI3OLJkN3hu"
   },
   "outputs": [],
   "source": [
    "np.random.seed(6)\n",
    "#random_bad_interview = my_df.loc[my_df[\"Participant\"] == 'pp89']\n",
    "random_bad_interview = my_df[index_bad_interviews].sample()\n",
    "random_good_interview = my_df[index_good_interviews].sample()\n",
    "random_good_interview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d41DVL185M0C"
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(4, 2, figsize=(15, 9))\n",
    "# Angry\n",
    "sns.kdeplot(data=good_bad_angry, hue=target_label, x='value', fill=True, ax=axes[0, 0], common_norm=False).set(title='angry')\n",
    "axes[0, 0].axvline(random_bad_interview['Sentiments_percentage'].iloc[0].get('angry'), color='red')\n",
    "axes[0, 0].axvline(random_good_interview['Sentiments_percentage'].iloc[0].get('angry'), color='green')\n",
    "\n",
    "# Calm\n",
    "sns.kdeplot(data=good_bad_calm, hue=target_label, x='value', fill=True, ax=axes[0, 1], common_norm=False).set(title='calm')\n",
    "axes[0, 1].axvline(random_bad_interview['Sentiments_percentage'].iloc[0].get('calm'), color='red')\n",
    "axes[0, 1].axvline(random_good_interview['Sentiments_percentage'].iloc[0].get('calm'), color='green')\n",
    "\n",
    "# Disgust\n",
    "sns.kdeplot(data=good_bad_disgust, hue=target_label, x='value', fill=True, ax=axes[1, 0], common_norm=False).set(title='disgust')\n",
    "axes[1, 0].axvline(random_bad_interview['Sentiments_percentage'].iloc[0].get('disgust'), color='red')\n",
    "axes[1, 0].axvline(random_good_interview['Sentiments_percentage'].iloc[0].get('disgust'), color='green')\n",
    "\n",
    "# Fear\n",
    "sns.kdeplot(data=good_bad_fear, hue=target_label, x='value', fill=True, ax=axes[1, 1], common_norm=False).set(title='fear')\n",
    "axes[1, 1].axvline(random_bad_interview['Sentiments_percentage'].iloc[0].get('fear'), color='red')\n",
    "axes[1, 1].axvline(random_good_interview['Sentiments_percentage'].iloc[0].get('fear'), color='green')\n",
    "\n",
    "# Happy\n",
    "sns.kdeplot(data=good_bad_happy, hue=target_label, x='value', fill=True, ax=axes[2, 0], common_norm=False).set(title='happy')\n",
    "axes[2, 0].axvline(random_bad_interview['Sentiments_percentage'].iloc[0].get('happy'), color='red')\n",
    "axes[2, 0].axvline(random_good_interview['Sentiments_percentage'].iloc[0].get('happy'), color='green')\n",
    "\n",
    "# Sad\n",
    "sns.kdeplot(data=good_bad_sad, hue=target_label, x='value', fill=True, ax=axes[2, 1], common_norm=False).set(title='sad')\n",
    "axes[2, 1].axvline(random_bad_interview['Sentiments_percentage'].iloc[0].get('sad'), color='red')\n",
    "axes[2, 1].axvline(random_good_interview['Sentiments_percentage'].iloc[0].get('sad'), color='green')\n",
    "\n",
    "# Suprise\n",
    "sns.kdeplot(data=good_bad_surprise, hue=target_label, x='value', fill=True, ax=axes[3, 0], common_norm=False).set(title='surprise')\n",
    "axes[3, 0].axvline(random_bad_interview['Sentiments_percentage'].iloc[0].get('surprise'), color='red')\n",
    "axes[3, 0].axvline(random_good_interview['Sentiments_percentage'].iloc[0].get('surprise'), color='green')\n",
    "\n",
    "plt.delaxes(ax=axes[3, 1])\n",
    "plt.tight_layout()\n",
    "\n",
    "red_patch = mpatches.Patch(color='red', label='bad example interview')\n",
    "green_patch = mpatches.Patch(color='green', label='good example interview')\n",
    "\n",
    "fig.legend(handles=[green_patch, red_patch], loc='lower right', fontsize='x-large')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CNiYmWaw5y27"
   },
   "outputs": [],
   "source": [
    "random_bad_interview = my_df.loc[my_df[\"Participant\"] == 'pp89']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o_edBoxcCCcH"
   },
   "outputs": [],
   "source": [
    "stdev_lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vm0y2siYN3fE"
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(4, 2, figsize=(15, 9))\n",
    "# Angry\n",
    "sns.kdeplot(data=good_bad_angry, hue=target_label, x='value', fill=True, ax=axes[0, 0], common_norm=False).set(title='angry')\n",
    "axes[0, 0].axvline(random_bad_interview['Sentiments_percentage'].iloc[0].get('angry'), color='red')\n",
    "#axes[0, 0].axvline(random_good_interview['Sentiments_percentage'].iloc[0].get('angry'), color='green')\n",
    "axes[0, 0].axvline(stdev_lines('angry')[0], color='blue')\n",
    "axes[0, 0].axvline(stdev_lines('angry')[1], color='blue')\n",
    "\n",
    "\n",
    "# Calm\n",
    "sns.kdeplot(data=good_bad_calm, hue=target_label, x='value', fill=True, ax=axes[0, 1], common_norm=False).set(title='calm')\n",
    "axes[0, 1].axvline(random_bad_interview['Sentiments_percentage'].iloc[0].get('calm'), color='red')\n",
    "#axes[0, 1].axvline(random_good_interview['Sentiments_percentage'].iloc[0].get('calm'), color='green')\n",
    "axes[0, 1].axvline(stdev_lines('calm')[0], color='blue')\n",
    "axes[0, 1].axvline(stdev_lines('calm')[1], color='blue')\n",
    "\n",
    "# Disgust\n",
    "sns.kdeplot(data=good_bad_disgust, hue=target_label, x='value', fill=True, ax=axes[1, 0], common_norm=False).set(title='disgust')\n",
    "axes[1, 0].axvline(random_bad_interview['Sentiments_percentage'].iloc[0].get('disgust'), color='red')\n",
    "#axes[1, 0].axvline(random_good_interview['Sentiments_percentage'].iloc[0].get('disgust'), color='green')\n",
    "axes[1, 0].axvline(stdev_lines('disgust')[0], color='blue')\n",
    "axes[1, 0].axvline(stdev_lines('disgust')[1], color='blue')\n",
    "\n",
    "# Fear\n",
    "sns.kdeplot(data=good_bad_fear, hue=target_label, x='value', fill=True, ax=axes[1, 1], common_norm=False).set(title='fear')\n",
    "axes[1, 1].axvline(random_bad_interview['Sentiments_percentage'].iloc[0].get('fear'), color='red')\n",
    "#axes[1, 1].axvline(random_good_interview['Sentiments_percentage'].iloc[0].get('fear'), color='green')\n",
    "axes[1, 1].axvline(stdev_lines('fear')[0], color='blue')\n",
    "axes[1, 1].axvline(stdev_lines('fear')[1], color='blue')\n",
    "\n",
    "# Happy\n",
    "sns.kdeplot(data=good_bad_happy, hue=target_label, x='value', fill=True, ax=axes[2, 0], common_norm=False).set(title='happy')\n",
    "axes[2, 0].axvline(random_bad_interview['Sentiments_percentage'].iloc[0].get('happy'), color='red')\n",
    "#axes[2, 0].axvline(random_good_interview['Sentiments_percentage'].iloc[0].get('happy'), color='green')\n",
    "axes[2, 0].axvline(stdev_lines('happy')[0], color='blue')\n",
    "axes[2, 0].axvline(stdev_lines('happy')[1], color='blue')\n",
    "\n",
    "\n",
    "# Sad\n",
    "sns.kdeplot(data=good_bad_sad, hue=target_label, x='value', fill=True, ax=axes[2, 1], common_norm=False).set(title='sad')\n",
    "axes[2, 1].axvline(random_bad_interview['Sentiments_percentage'].iloc[0].get('sad'), color='red')\n",
    "#axes[2, 1].axvline(random_good_interview['Sentiments_percentage'].iloc[0].get('sad'), color='green')\n",
    "axes[2, 1].axvline(stdev_lines('sad')[0], color='blue')\n",
    "axes[2, 1].axvline(stdev_lines('sad')[1], color='blue')\n",
    "\n",
    "# Suprise\n",
    "sns.kdeplot(data=good_bad_surprise, hue=target_label, x='value', fill=True, ax=axes[3, 0], common_norm=False).set(title='surprise')\n",
    "axes[3, 0].axvline(random_bad_interview['Sentiments_percentage'].iloc[0].get('surprise'), color='red')\n",
    "#axes[3, 0].axvline(random_good_interview['Sentiments_percentage'].iloc[0].get('surprise'), color='green')\n",
    "axes[3, 0].axvline(stdev_lines('surprise')[0], color='blue')\n",
    "axes[3, 0].axvline(stdev_lines('surprise')[1], color='blue')\n",
    "\n",
    "\n",
    "plt.delaxes(ax=axes[3, 1])\n",
    "plt.tight_layout()\n",
    "\n",
    "red_patch = mpatches.Patch(color='red', label='Your score')\n",
    "blue_patch = mpatches.Patch(color='blue', label='one std for good interviews')\n",
    "\n",
    "fig.legend(handles=[blue_patch, red_patch], loc='lower right', fontsize='x-large')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4MOruh2BN3cU"
   },
   "outputs": [],
   "source": [
    "good_bad_surprise.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xDp22vF2SmAn"
   },
   "outputs": [],
   "source": [
    "random_bad_interview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y7soWknMwnDm"
   },
   "source": [
    "### Correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z4y6bh9twpVm"
   },
   "outputs": [],
   "source": [
    "corr_list = {}\n",
    "n = my_df.shape[0]\n",
    "for sentiment in ['angry', 'calm', 'disgust', 'fear', 'happy', 'sad', 'surprise']:\n",
    "  s_arr = np.zeros(n)\n",
    "  for i in range(n):\n",
    "    s_arr[i] = ( my_df[\"Sentiments_percentage\"].values[i][sentiment])\n",
    "  corr_list[sentiment] = s_arr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GQW_zY78BTHg"
   },
   "outputs": [],
   "source": [
    "import scipy\n",
    "\n",
    "corr_values = {}\n",
    "for sentiment in ['angry', 'calm', 'disgust', 'fear', 'happy', 'sad', 'surprise']:\n",
    "  data1 = corr_list[sentiment]\n",
    "  df2 = my_df['RecommendHiring']\n",
    "  #corr, _ = pearsonr(data1, data2)\n",
    "  df1 = pd.DataFrame(data1,columns=['value'])\n",
    "  #df2 = pd.DataFrame(data2,columns=['RecommendHiring'])\n",
    "\n",
    "  #new_df = pd.concat([df1,df2],axis=1)\n",
    "\n",
    "  corr_values[sentiment] = scipy.stats.pearsonr(df2.values, df1.values.reshape(1,-1)[0])#new_df.corr(method ='pearson')['value'].values[1]\n",
    "  #print('Pearsons correlation: %.3f' % corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w3-G1l_SujAc"
   },
   "outputs": [],
   "source": [
    "corr_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "and1jTedv_ke"
   },
   "outputs": [],
   "source": [
    "good_bad_happy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w3Ax-lOvvKBJ"
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QKUJwxIUvNZz"
   },
   "outputs": [],
   "source": [
    "#my_df.to_csv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TYRZpZbPvOUh"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rVjMSxTB6FxR"
   },
   "source": [
    "# Explanations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Al2mFTNsN3ZJ"
   },
   "outputs": [],
   "source": [
    "!pip install shap\n",
    "!pip install lime\n",
    "import lime\n",
    "import lime.lime_tabular\n",
    "import shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZHk1lLq5m6wI"
   },
   "outputs": [],
   "source": [
    "tmp_df = pd.DataFrame(x_test, columns=meaningful_cols)\n",
    "shap_values = explainer(tmp_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6GQP15I76gb5"
   },
   "source": [
    "## Lime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "esXoIJ6p6a_C"
   },
   "outputs": [],
   "source": [
    "clf = pickle_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vapWzZk66a7T"
   },
   "outputs": [],
   "source": [
    "explainer_lime = lime.lime_tabular.LimeTabularExplainer(x_train, feature_names=meaningful_cols, class_names=['angry', 'calm', 'disgust', 'fear', 'happy', 'sad', 'surprise'], verbose=True, mode='classification')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PNp3mCWg6ahR"
   },
   "outputs": [],
   "source": [
    "exp = explainer_lime.explain_instance(x_test[2], clf.predict_proba, num_features=5, top_labels=3)\n",
    "exp.show_in_notebook(show_table=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hqLs-49E6cHJ"
   },
   "outputs": [],
   "source": [
    "pkl_filename_lime = dir + \"/svc_model_lime_sentiment.pkl\"\n",
    "with open(pkl_filename_lime, 'wb') as file:\n",
    "    dill.dump(explainer_lime, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xu7ZvlPx6cEZ"
   },
   "outputs": [],
   "source": [
    "pkl_filename_lime = dir + \"/svc_model_lime_sentiment.pkl\"\n",
    "with open(pkl_filename_lime, 'rb') as file:\n",
    "    pickle_lime = dill.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VDbfXT9lYNX0"
   },
   "outputs": [],
   "source": [
    "explainer_lime = pickle_lime"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "sentiment_analysis_model.ipynb",
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

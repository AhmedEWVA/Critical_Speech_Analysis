{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce3686ba",
   "metadata": {},
   "source": [
    "# Textual preprocessing and simple feature extraction\n",
    "***\n",
    "## Workflow\n",
    "1. Imports\n",
    "2. Data cleaning\n",
    "3. Simple feature creation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d027947a",
   "metadata": {},
   "source": [
    "## 1. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "66d31682",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import nltk\n",
    "import re\n",
    "import librosa\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1279b703",
   "metadata": {},
   "outputs": [],
   "source": [
    "name_prefix = 'data/interview_transcripts_by_turkers'\n",
    "dataset_csv_name = name_prefix + '.csv'\n",
    "\n",
    "df = pd.read_csv(dataset_csv_name, header=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ebb15f1",
   "metadata": {},
   "source": [
    "## 2. Data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7c9808ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_interview_indicators(string):\n",
    "    out = re.sub(\"Interviewer:[a-zA-Z0-9\\.\\?\\!\\ \\']*\\|\", \"\", string) # remove interviewer from the text\n",
    "    out = re.sub(\"Interviewer:[a-zA-Z0-9\\.\\?\\!\\ \\']*\", \"\", out) # remove last interviewer statement from text\n",
    "    out = re.sub(\"Interviewee:|\\|Interviewee:\", \"\", out) # remove Interviewee indicator from text\n",
    "    out = re.sub(\"\\|\", \"\", out) # remove remaining |\n",
    "    out = re.sub(\"\\ \\ \", \" \", out) # remove double white spaces (still some in there)\n",
    "    out = re.sub(\"â€™|'\", \"\", out) # remove double white spaces (still some in there)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "563f5e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuations(text):\n",
    "    for punctuation in string.punctuation:\n",
    "        text = text.replace(punctuation, '')\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "313ebc0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the duration of the audio clips\n",
    "\n",
    "def get_duration_lib(name):\n",
    "    name = name.upper()\n",
    "    try:\n",
    "        sec = librosa.get_duration(filename=f'Audio/{name}.wav')\n",
    "    except:\n",
    "        sec = 0\n",
    "    \n",
    "    return sec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9f2185ef",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "transformed_interviews = df.copy()\n",
    "transformed_interviews[1] = transformed_interviews[1].transform(remove_interview_indicators)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcb6d48b",
   "metadata": {},
   "source": [
    "average number of words spoken per minute, the average number of unique words per minute, count of unique words in the transcript, and the number of filler words used per minute. (Leveraging Multimodal Behavioral Analytics for Automated Job Interview Performance Assessment and Feedback)\n",
    "\n",
    "wps Words per second, uwps Unique words per second, fwps Filler words per second, wc Total number of words, uwc Total number of unique words (Automated Analysis and Prediction of Job Interview Performance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0faa62a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_interviews[2] = transformed_interviews[0].transform(get_duration_lib)\n",
    "transformed_interviews['remove_punctation'] = transformed_interviews[1].transform(remove_punctuations)\n",
    "transformed_interviews['tokenize'] = transformed_interviews['remove_punctation'].transform(nltk.word_tokenize)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5a020c9",
   "metadata": {},
   "source": [
    "Number of filler words and non fluencies LIWC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c937090d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# words per second \n",
    "transformed_interviews['wps'] = transformed_interviews['tokenize'].transform(len) / transformed_interviews[2]\n",
    "transformed_interviews['wc'] = transformed_interviews['tokenize'].transform(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "6c8019ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>remove_punctation</th>\n",
       "      <th>tokenize</th>\n",
       "      <th>wps</th>\n",
       "      <th>wc</th>\n",
       "      <th>stemming</th>\n",
       "      <th>uwc</th>\n",
       "      <th>uwps</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>p1</td>\n",
       "      <td>Im pretty good. ok uhm so have you looked at ...</td>\n",
       "      <td>196.992</td>\n",
       "      <td>Im pretty good ok uhm so have you looked at m...</td>\n",
       "      <td>[Im, pretty, good, ok, uhm, so, have, you, loo...</td>\n",
       "      <td>2.695541</td>\n",
       "      <td>531</td>\n",
       "      <td>[im, pretti, good, ok, uhm, so, have, you, loo...</td>\n",
       "      <td>231</td>\n",
       "      <td>1.172636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>p10</td>\n",
       "      <td>Great how about you? Im a little [???] by the...</td>\n",
       "      <td>426.000</td>\n",
       "      <td>Great how about you Im a little  by the resur...</td>\n",
       "      <td>[Great, how, about, you, Im, a, little, by, th...</td>\n",
       "      <td>2.389671</td>\n",
       "      <td>1018</td>\n",
       "      <td>[great, how, about, you, im, a, littl, by, the...</td>\n",
       "      <td>303</td>\n",
       "      <td>0.711268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>p11</td>\n",
       "      <td>Uhh Im a junior at MIT uhh Im double majoring...</td>\n",
       "      <td>271.992</td>\n",
       "      <td>Uhh Im a junior at MIT uhh Im double majoring...</td>\n",
       "      <td>[Uhh, Im, a, junior, at, MIT, uhh, Im, double,...</td>\n",
       "      <td>2.441248</td>\n",
       "      <td>664</td>\n",
       "      <td>[uhh, im, a, junior, at, mit, uhh, im, doubl, ...</td>\n",
       "      <td>228</td>\n",
       "      <td>0.838260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>p12</td>\n",
       "      <td>Im good how are you? Ok so Im a Junior at MIT...</td>\n",
       "      <td>204.984</td>\n",
       "      <td>Im good how are you Ok so Im a Junior at MIT ...</td>\n",
       "      <td>[Im, good, how, are, you, Ok, so, Im, a, Junio...</td>\n",
       "      <td>2.995356</td>\n",
       "      <td>614</td>\n",
       "      <td>[im, good, how, are, you, ok, so, im, a, junio...</td>\n",
       "      <td>215</td>\n",
       "      <td>1.048862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>p13</td>\n",
       "      <td>Good. Ok umm Im currently a junior at M.I.T. ...</td>\n",
       "      <td>294.000</td>\n",
       "      <td>Good Ok umm Im currently a junior at MIT stud...</td>\n",
       "      <td>[Good, Ok, umm, Im, currently, a, junior, at, ...</td>\n",
       "      <td>1.914966</td>\n",
       "      <td>563</td>\n",
       "      <td>[good, ok, umm, im, current, a, junior, at, mi...</td>\n",
       "      <td>211</td>\n",
       "      <td>0.717687</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0                                                  1        2  \\\n",
       "0   p1   Im pretty good. ok uhm so have you looked at ...  196.992   \n",
       "1  p10   Great how about you? Im a little [???] by the...  426.000   \n",
       "2  p11   Uhh Im a junior at MIT uhh Im double majoring...  271.992   \n",
       "3  p12   Im good how are you? Ok so Im a Junior at MIT...  204.984   \n",
       "4  p13   Good. Ok umm Im currently a junior at M.I.T. ...  294.000   \n",
       "\n",
       "                                   remove_punctation  \\\n",
       "0   Im pretty good ok uhm so have you looked at m...   \n",
       "1   Great how about you Im a little  by the resur...   \n",
       "2   Uhh Im a junior at MIT uhh Im double majoring...   \n",
       "3   Im good how are you Ok so Im a Junior at MIT ...   \n",
       "4   Good Ok umm Im currently a junior at MIT stud...   \n",
       "\n",
       "                                            tokenize       wps    wc  \\\n",
       "0  [Im, pretty, good, ok, uhm, so, have, you, loo...  2.695541   531   \n",
       "1  [Great, how, about, you, Im, a, little, by, th...  2.389671  1018   \n",
       "2  [Uhh, Im, a, junior, at, MIT, uhh, Im, double,...  2.441248   664   \n",
       "3  [Im, good, how, are, you, Ok, so, Im, a, Junio...  2.995356   614   \n",
       "4  [Good, Ok, umm, Im, currently, a, junior, at, ...  1.914966   563   \n",
       "\n",
       "                                            stemming  uwc      uwps  \n",
       "0  [im, pretti, good, ok, uhm, so, have, you, loo...  231  1.172636  \n",
       "1  [great, how, about, you, im, a, littl, by, the...  303  0.711268  \n",
       "2  [uhh, im, a, junior, at, mit, uhh, im, doubl, ...  228  0.838260  \n",
       "3  [im, good, how, are, you, ok, so, im, a, junio...  215  1.048862  \n",
       "4  [good, ok, umm, im, current, a, junior, at, mi...  211  0.717687  "
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# unique words\n",
    "sno = nltk.stem.SnowballStemmer('english')\n",
    "\n",
    "transformed_interviews['stemming'] = transformed_interviews['tokenize'].transform(lambda words: [sno.stem(word) for word in words])\n",
    "transformed_interviews['stemming']\n",
    "\n",
    "transformed_interviews['uwc'] = transformed_interviews['stemming'].transform(lambda words: len(set(words)))\n",
    "transformed_interviews['uwps'] = transformed_interviews['uwc'] / transformed_interviews[2]\n",
    "transformed_interviews.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "9d5270b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_interviews.rename(columns = {0:'Person', 1:'text_unprocessed', 2:'interview_length'}, inplace = True)\n",
    "output = transformed_interviews[['Person', 'text_unprocessed', 'tokenize', 'stemming', 'interview_length', 'wc', \\\n",
    "                                 'wps', 'uwc', 'uwps', 'remove_punctation']]\n",
    "output = output.drop('remove_punctation',  axis=1)\n",
    "output.to_csv('wordcount_uniquewordcount.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
